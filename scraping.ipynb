{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping des pages:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titre: La Banque de France s’associe à Acceo pour renforcer l’accessibilité de ses services aux personnes sourdes et malentendantes\n",
      "Lien: https://www.banque-france.fr/fr/espace-presse/communiques-de-presse/la-banque-de-france-sassocie-acceo-pour-renforcer-laccessibilite-de-ses-services-aux-personnes\n",
      "https://www.banque-france.fr/fr/actualites/la-banque-de-france-renforce-laccessibilite-de-ses-services-aux-personnes-sourdes-ou-malentendantes\n",
      "Date de publication: 2024-02-06\n",
      "Contenu: La Banque de France met à la disposition des personnes sourdes et malentendantes un service de traduction instantanée en langue des signes et en textes en intégrant la solution Acceo. \n",
      "Grace à cette interface, les personnes sourdes et malentendantes peuvent aisément communiquer avec les agents de la Banque de France pour toutes questions liées au surendettement, au droit au compte, aux fichiers d’incidents, ou pour toutes autres demandes relatives à la règlementation et aux pratiques bancaires ou d’assurance (frais bancaires, crédits, moyens de paiement, usurpation d’identité etc).\n",
      " \n",
      "Acceo permet un accès à distance et en temps réel à une plateforme d’opérateurs spécialisés en :\n",
      "-    Transcription Instantanée de la Parole (TIP) – propos du conseiller sous-titrés en temps réel.\n",
      "-    Visio-interprétation en Langue des Signes Française (LSF) – traduction simultanée à l’oral et en langue des signes de la conversation entre conseiller et particulier.\n",
      "-    Visio-codage Langue française Parlée Complétée (LPC) – codage instantané des propos du conseiller sur l’écran du particulier\n",
      "Ce service gratuit et facile d’utilisation permet d’interagir avec la Banque de France de 9 heures à 17 heures 30 :\n",
      "-    Par téléphone via la plateforme dédiée, notamment accessible depuis son site Internet.\n",
      "-    En face à face à ses guichets présents dans l’ensemble des départements, grâce à  l’application Acceo disponible sur smartphone, tablette et ordinateur. \n",
      " \n",
      "\n",
      "Le bouton d'acceptation des cookies n'a pas été trouvé dans le temps imparti.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping des pages:   7%|▋         | 1/15 [00:14<03:16, 14.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titre: Le livret d'épargne populaire (LEP)\n",
      "Lien: https://www.banque-france.fr/fr/a-votre-service/particuliers/connaitre-pratiques-bancaires-assurance/epargne/livrets-epargne-populaire\n",
      "https://www.banque-france.fr/fr/actualites/lep-nouveau-taux-de-remuneration-compter-du-1er-fevrier-2024\n",
      "Date de publication: 2024-02-05\n",
      "Contenu: Le livret d’épargne populaire est un livret d’épargne bancaire réglementé, mieux rémunéré que le livret A et destiné aux personnes disposant de revenus modestes. Il permet de mettre de l’argent de côté et de se constituer une épargne.\n",
      "Créé en 1982, le livret d’épargne populaire (LEP) a pour objectif d’aider les personnes aux revenus les plus modestes à épargner dans des conditions permettant de maintenir leur pouvoir d’achat. \n",
      "Retrouvez dans cet article les conditions et caractéristiques du LEP.\n",
      "Conditions d'ouverture et de détention\n",
      "Pour ouvrir un LEP, vous pouvez solliciter un établissement qui est habilité à le proposer, c’est-à-dire ayant signé une convention avec la Caisse des Dépôts et Consignation (CDC). La majorité des banques traditionnelles proposent le LEP contrairement aux banques en ligne. \n",
      "Pour ouvrir un LEP il faut avoir son domicile fiscal en France et respecter certains plafonds de revenus.\n",
      "Il est possible d’avoir un LEP par personne et deux LEP maximum par foyer fiscal (un par contribuable et un pour son partenaire lié par un PACS). Il n’est pas possible pour un majeur rattaché au foyer fiscal d’un tiers, par exemple de ses parents, de bénéficier d’un LEP, il lui faudra détenir son propre avis d’imposition. \n",
      "Pour ouvrir un LEP, il faut respecter des conditions de revenus. Le revenu fiscal de référence N-2 ou N-1, indiqué sur votre avis d’imposition, ne doit pas excéder un certain seuil. Ce seuil est aussi fonction de votre composition familiale et de votre lieu de résidence. Il est révisé annuellement.\n",
      "Par exemple, pour pouvoir ouvrir un LEP en 2024, votre revenu fiscal de référence N-2 ou N-1, figurant sur votre avis d’imposition 2023 sur les revenus de 2022 ou sur votre avis d'imposition 2024 sur les revenus 2023, ne doit pas excéder les seuils suivants si vous résidez en métropole :\n",
      " \n",
      "22 419 €\n",
      "28 406 €\n",
      "34 393 €\n",
      "40 380 €\n",
      "46 367 €\n",
      "52 354 €\n",
      "58 341 €\n",
      "5 987 €\n",
      "Si vous résidez en outre-mer, des seuils différents s’appliquent. Ils sont plus élevés que ceux utilisés en métropole. Retrouvez tous les plafonds de revenus pour ouvrir un livret d’épargne populaire.\n",
      "Depuis mars 2021, l'établissement bancaire gestionnaire du LEP peut interroger directement l'administration fiscale afin de savoir si les conditions d’éligibilité sont remplies par le titulaire du compte ou par la personne qui en demande l'ouverture. Si l’administration fiscale est en mesure de lui répondre, le titulaire du LEP n’aura pas besoin de fournir son avis d’imposition N-2 ou N-1.\n",
      "En pratique, les établissements bancaires vous demanderont généralement de présenter votre avis d’imposition le plus récent en votre possession, à savoir votre avis d’imposition N-2 ou N-1.\n",
      "Chaque année, il est nécessaire de justifier que vous respectez toujours les conditions de revenus. Pour cela, la banque pourra interroger directement l’administration fiscale ou vous demander de présenter votre avis d’imposition ou avis de situation déclarative à l’impôt sur le revenu. \n",
      "À noter que si pendant un an vos revenus sont supérieurs au plafond, vous pourrez conserver votre LEP à condition que l’année suivante vos revenus repassent sous la limite de plafond de revenus correspondant à votre situation.\n",
      "Pour ouvrir un livret d'épargne populaire, il faut effectuer un versement initial d’un montant minimal de 30 euros.\n",
      "Les versements effectués sur un livret d'épargne populaire ne peuvent porter le montant inscrit sur ce livret au-delà du plafond de 10 000 euros.  Ce montant est composé des versements que vous avez effectués et des intérêts déjà capitalisés.\n",
      "Une fois ce plafond atteint, le LEP continue de porter intérêt mais tout versement supplémentaire est interdit.\n",
      "Les versements et retraits sur le LEP sont libres. Leur montant minimal est généralement de 10 euros.\n",
      "Les versements peuvent être effectués en espèces, par chèque ou virement depuis un compte de dépôt au nom du titulaire du LEP.\n",
      "Les retraits peuvent être effectués en espèces (retrait au guichet de votre banque ou dans un distributeur appartenant au réseau de votre banque si une carte de retrait est associée au LEP), par virement vers un compte au nom du titulaire du LEP, ou encore par un chèque de banque.\n",
      "Aucune disposition ne vous oblige à ouvrir dans la même banque un compte de dépôt en complément de votre LEP.\n",
      "Le solde du LEP peut être de 0 €. En revanche, le LEP ne peut pas présenter un solde débiteur.\n",
      "La rémunération du LEP est fixée par l’État et est de 5 % à compter du 1er février 2024. (Auparavant 6 % depuis le 1er août 2023).\n",
      "Les intérêts sont calculés par quinzaine et capitalisés au 31 décembre de chaque année.\n",
      "Le LEP n’est pas fiscalisé. Les intérêts sont nets d’impôts et de prélèvements sociaux. \n",
      "Un relevé de compte est délivré sur la période définie au contrat dès lors qu’une opération a été réalisée sur le LEP.\n",
      "Le transfert est possible entre établissements habilités à proposer le LEP et nécessite l’accord des deux banques.\n",
      "Certaines banques peuvent facturer des frais de transfert.\n",
      "Vous pouvez clôturer votre LEP gratuitement et à tout moment, sur demande auprès de l’établissement teneur du compte que ce soit par courrier ou au guichet de votre banque.  \n",
      "En cas de clôture du compte en cours d'année, les intérêts acquis sont crédités au jour de la clôture. Les fonds seront versés sur un autre compte ouvert à votre nom ouvert dans la même banque ou mis sur un compte d’attente.\n",
      "En fonction de l’établissement, il est possible qu’il y ait un délai nécessaire au dénouement des opérations avant que la clôture ne soit effective. Vous pouvez consulter les conditions générales d’ouverture du LEP pour en savoir plus sur ce point.\n",
      "Par ailleurs, si vous souhaitez rouvrir un LEP dans un autre établissement, le nouvel établissement peut interroger l’administration fiscale afin de savoir si les conditions d’éligibilités sont bien remplies et notamment si aucun LEP n’est ouvert dans une autre banque. Le délai entre la clôture et l’ouverture d’un nouveau LEP peut dépendre du délai de mise à jour des informations entre les établissements bancaires et les services de l’administration fiscale. Vous pourrez dans ce cas fournir au nouvel établissement la preuve de la clôture de votre LEP.\n",
      "Si vous ne respectez plus, pour la deuxième année consécutive, les conditions d’éligibilité du LEP, vous devez en informer votre banque et en demander sa clôture.\n",
      "La clôture du LEP est automatique si vous dépassez pendant 2 ans consécutifs le plafond de ressources correspondant à votre situation.\n",
      "La clôture sera effectuée au plus tard le 30 avril de la deuxième année consécutive où vous avez cessé de remplir les conditions de détention du LEP. Les fonds seront versés sur un autre compte à votre nom ouvert dans la même banque ou mis sur un compte d’attente.\n",
      "Pour en savoir plus\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping des pages:  87%|████████▋ | 13/15 [00:33<00:02,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article antérieur à 2023 détecté : 2022-11-15, arrêt du scraping. Lien de l'article : https://www.banque-france.fr/fr/interventions-gouverneur/europe-et-japon-nos-forces-et-nos-defis-communs-dans-un-monde-hautement-incertain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping des pages:  93%|█████████▎| 14/15 [00:34<00:01,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article antérieur à 2023 détecté : 2022-10-06, arrêt du scraping. Lien de l'article : https://www.banque-france.fr/fr/governors-interventions/la-situation-macroeconomique-actuelle-dans-la-zone-euro-et-les-questions-de-politique-monetaire\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping des pages: 100%|██████████| 15/15 [00:35<00:00,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article antérieur à 2023 détecté : 2022-01-04, arrêt du scraping. Lien de l'article : https://www.banque-france.fr/fr/interventions-gouverneur/voeux-du-gouverneur-de-la-banque-de-france\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "import re\n",
    "import sqlite3\n",
    "import time\n",
    "from datetime import datetime, MINYEAR\n",
    "from dateutil import parser\n",
    "from tqdm import tqdm\n",
    "import locale\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import requests\n",
    "\n",
    "\n",
    "from googletrans import Translator, LANGUAGES\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class BaseScraper:\n",
    "    def __init__(self, db_name, table_name):\n",
    "        self.db_name = db_name\n",
    "        self.table_name = table_name\n",
    "        self.conn = sqlite3.connect(self.db_name)\n",
    "        self.cursor = self.conn.cursor()\n",
    "\n",
    "        self.cursor.execute(f'''\n",
    "            CREATE TABLE IF NOT EXISTS {self.table_name} (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                title TEXT NOT NULL,\n",
    "                date DATE,\n",
    "                link TEXT NOT NULL,\n",
    "                source TEXT NOT NULL,\n",
    "                content TEXT\n",
    "            )\n",
    "        ''')\n",
    "        self.conn.commit()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.conn.close()\n",
    "\n",
    "    def insert_into_db(self, article):\n",
    "        self.cursor.execute(f'''\n",
    "            INSERT INTO {self.table_name} (title, date, link, source, content) VALUES (?, ?, ?, ?, ?)\n",
    "        ''', (article['Titre'], article['Date'], article['URL'], article['source'], article['Contenu']))\n",
    "        self.conn.commit()\n",
    "\n",
    "    def scrape_site(self):\n",
    "        raise NotImplementedError(\"La méthode doit être implémentée dans chaque sous-classe\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def fusionner_bases_de_donnees(chemins_bdd_existants, chemin_nouvelle_bdd):\n",
    "        # Créer une nouvelle base de données et la table commune\n",
    "        conn_nouvelle_bdd = sqlite3.connect(chemin_nouvelle_bdd)\n",
    "        cursor_nouvelle_bdd = conn_nouvelle_bdd.cursor()\n",
    "        cursor_nouvelle_bdd.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS articles (\n",
    "                id INTEGER,\n",
    "                title TEXT NOT NULL,\n",
    "                date DATE,\n",
    "                link TEXT NOT NULL,\n",
    "                source TEXT,\n",
    "                content TEXT\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        # Lecture et insertion des données de chaque base de données existante\n",
    "        for chemin in chemins_bdd_existants:\n",
    "            # Obtenir le nom de la table à partir de la base de données existante\n",
    "            conn = sqlite3.connect(chemin)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "            nom_table = cursor.fetchone()[0]\n",
    "\n",
    "            # Lire les données de la table\n",
    "            cursor.execute(f\"SELECT * FROM {nom_table}\")\n",
    "            lignes = cursor.fetchall()\n",
    "            conn.close()\n",
    "\n",
    "            # Insertion des données dans la nouvelle base de données\n",
    "            cursor_nouvelle_bdd.executemany(\"INSERT INTO articles(id, title, date, link, source, content) VALUES (?, ?, ?, ?, ?, ?)\", lignes)\n",
    "\n",
    "        # Valider les changements et fermer la nouvelle base de données\n",
    "        conn_nouvelle_bdd.commit()\n",
    "        conn_nouvelle_bdd.close()\n",
    "\n",
    "class AmfScraper(BaseScraper):\n",
    "    def __init__(self, db_name=\"amf.db\"):\n",
    "        super().__init__(db_name, 'amf_articles')\n",
    "        self.url_base = \"https://www.amf-france.org/fr/actualites-publications/la-une/toutes-les-actualites-et-publications\"\n",
    "        self.options = webdriver.ChromeOptions()\n",
    "        self.options.add_argument('--headless')\n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "        self.driver = webdriver.Chrome(options=self.options)\n",
    "        self.consecutive_duplicates = 0  # Compteur de doublons consécutifs\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        super().__del__()\n",
    "        self.driver.quit()\n",
    "\n",
    "    def is_duplicate(self, article):\n",
    "        # Vérifier si l'article existe déjà dans la base de données\n",
    "        self.cursor.execute('SELECT COUNT(*) FROM amf_articles WHERE link = ?', (article['URL'],))\n",
    "        return self.cursor.fetchone()[0] > 0\n",
    "    \n",
    "    def delete_last_articles(self, num_articles=1):\n",
    "        try:\n",
    "            for _ in range(num_articles):\n",
    "                self.cursor.execute(f'''\n",
    "                    DELETE FROM {self.table_name}\n",
    "                    WHERE id = (SELECT MIN(id) FROM {self.table_name})\n",
    "                ''')\n",
    "                self.conn.commit()\n",
    "                print(\"Dernier article supprimé de la table.\")\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"Erreur lors de la suppression de l'article : {e}\")\n",
    "\n",
    "    def scrape_site(self):\n",
    "        self.driver.get(self.url_base)\n",
    "        locale.setlocale(locale.LC_TIME, 'fr_FR')\n",
    "\n",
    "        full_xpath = \"/html/body/div[2]/div[1]/main/div[3]/div/div[2]/div/div[2]/div/div[2]/div/div[2]/table\"\n",
    "        WebDriverWait(self.driver, 30).until(EC.visibility_of_element_located((By.XPATH, full_xpath)))\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                current_page = self.driver.find_element(By.CLASS_NAME, 'paginate_button.current').text\n",
    "                print(f\"Page actuelle : {current_page}\")\n",
    "\n",
    "                try:\n",
    "                    rows = self.driver.find_elements(By.XPATH, f\"{full_xpath}/tbody/tr\")\n",
    "                    urls = [row.find_element(By.XPATH, \".//td[3]/a\").get_attribute('href') for row in rows]\n",
    "\n",
    "                    for url in tqdm(urls, desc=\"Traitement des articles\"):\n",
    "                        self.driver.execute_script(\"window.open();\")\n",
    "                        self.driver.switch_to.window(self.driver.window_handles[1])\n",
    "                        self.driver.get(url)\n",
    "\n",
    "                        article = {'URL': url}\n",
    "                        article['source']= \"AMF\"\n",
    "                        try:\n",
    "                            article['Titre'] = self.driver.find_element(By.CSS_SELECTOR, '.like-h1').text\n",
    "                        except:\n",
    "                            article['Titre'] = \"\"\n",
    "\n",
    "                        try:\n",
    "                            article['Date'] = self.driver.find_element(By.CSS_SELECTOR, '.date').text\n",
    "                            raw_date = datetime.strptime(article['Date'], \"%d %B %Y\")\n",
    "                            if raw_date.year < 2023:\n",
    "                                print(\"Date antérieure à 2023. Arrêt du scraping.\")\n",
    "                                return\n",
    "                            article['Date'] = raw_date.strftime(\"%Y-%m-%d\")\n",
    "                        except ValueError:\n",
    "                            print(f\"Format de date non reconnu : {raw_date}\")\n",
    "                            continue\n",
    "\n",
    "                        try:\n",
    "                            content_divs = self.driver.find_elements(By.CSS_SELECTOR, \n",
    "                                                                      \"div.paragraph.paragraph--type--wysiwyg.paragraph--view-mode--default, div.intro\")\n",
    "                            article['Contenu'] = ' '.join([div.text for div in content_divs])\n",
    "                        except:\n",
    "                            article['Contenu'] = \"\"\n",
    "\n",
    "                        # Vérifier si l'article existe déjà dans la base de données\n",
    "                        if not self.is_duplicate(article):\n",
    "                            self.insert_into_db(article)\n",
    "                            self.consecutive_duplicates = 0 \n",
    "                        else:\n",
    "                            print(f\"Article déjà existant dans la base de données : {article['URL']}\")\n",
    "                            self.consecutive_duplicates += 1\n",
    "\n",
    "                        self.driver.close()\n",
    "                        self.driver.switch_to.window(self.driver.window_handles[0])\n",
    "\n",
    "                        # Condition pour arrêter le scraping si deux doublons consécutifs sont rencontrés\n",
    "                        if self.consecutive_duplicates >= 2:\n",
    "                            print(\"Deux doublons consécutifs rencontrés. Arrêt du scraping.\")\n",
    "                            return\n",
    "                        \n",
    "                except NoSuchElementException:\n",
    "                    print(\"Impossible de trouver les lignes ou URLs sur la page principale.\")\n",
    "                \n",
    "                try:\n",
    "                    next_button = self.driver.find_element(By.ID, 'DataTables_Table_0_next')\n",
    "                    if 'disabled' not in next_button.get_attribute('class'):\n",
    "                        self.driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                    else:\n",
    "                        print(\"next page impossible\")\n",
    "                        break\n",
    "                except:\n",
    "                    print(\"erreur bouton next page\")\n",
    "\n",
    "                WebDriverWait(self.driver, 30).until(\n",
    "                    EC.staleness_of(next_button)\n",
    "                )\n",
    "\n",
    "        except:\n",
    "            print(\"Erreur lors du scraping.\")\n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "\n",
    "class CnilScraper(BaseScraper):\n",
    "    def __init__(self, start_date='2023-01-01', end_date=None, db_name=\"cnil.db\"):\n",
    "        super().__init__(db_name, 'cnil_articles')\n",
    "        self.start_date = datetime(MINYEAR, 1, 1) if start_date in ['Max', 'Earliest', ''] else datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        self.end_date = datetime.now() if end_date is None else datetime.strptime(end_date, \"%d-%m-%Y\")\n",
    "\n",
    "    def scrape_site(self):\n",
    "        session = requests.Session()\n",
    "        retry = Retry(connect=3, backoff_factor=0.5)\n",
    "        adapter = HTTPAdapter(max_retries=retry)\n",
    "        session.mount('http://', adapter)\n",
    "        session.mount('https://', adapter)\n",
    "\n",
    "        base_url = 'https://www.cnil.fr/'\n",
    "\n",
    "        # Réglage de la locale pour interpréter correctement les noms de mois en français\n",
    "        locale.setlocale(locale.LC_TIME, 'fr_FR')\n",
    "\n",
    "        try:\n",
    "            for i in tqdm(range(86), desc=\"Scraping des articles\"):\n",
    "                \n",
    "                cnil_url = f'{base_url}fr/actualites?page={i}'\n",
    "                try:\n",
    "                    page = session.get(cnil_url)\n",
    "                    soup = bs(page.text, \"lxml\")\n",
    "                    view = soup.find(\"div\", class_=\"view-content\")\n",
    "                    articles = view.find_all(\"div\", class_=\"views-row\")\n",
    "\n",
    "                    compteur_doublons = 0  # Initialisation du compteur de doublons\n",
    "                    for article in articles:\n",
    "                        link = base_url + article.find(\"a\")[\"href\"]\n",
    "                        if self.is_duplicate(link):\n",
    "                            print(f\"Article déjà scrapé trouvé, lien : {link}.\")\n",
    "                            compteur_doublons += 1  # Incrémentation du compteur de doublons\n",
    "                            if compteur_doublons >= 2:\n",
    "                                print(\"Deux doublons consécutifs trouvés. Arrêt du scraping.\")\n",
    "                                return  # Arrêt du scraping après deux doublons consécutifs\n",
    "                            continue\n",
    "                        else:\n",
    "                            compteur_doublons = 0  # Réinitialisation du compteur si l'article n'est pas un doublon\n",
    "                        \n",
    "                        article = {'URL': link}\n",
    "                        article['source'] = \"CNIL\"\n",
    "                        page_article = session.get(link)\n",
    "                        soup_article = bs(page_article.text, \"lxml\")\n",
    "                        raw_date = soup_article.find(\"div\", class_=[\"ctn-gen-auteur\", \"field-name-field-date-de-publication\"]).text.strip()\n",
    "                        try:\n",
    "                            raw_date = datetime.strptime(raw_date, \"%d %B %Y\")\n",
    "                            article['Date'] = raw_date.strftime(\"%Y-%m-%d\")  # Format ISO pour stockage\n",
    "                        except ValueError:\n",
    "                            print(f\"Format de date non reconnu : {raw_date}\")\n",
    "                            continue\n",
    "                        content = soup_article.find(\"div\", class_=\"ctn-gen\")\n",
    "                        paragraphs = content.find_all(\"p\")\n",
    "                        article['Contenu'] = \" \".join([p.text for p in paragraphs])\n",
    "                        article['Titre'] = soup_article.find(\"h1\").text\n",
    "\n",
    "                        try : \n",
    "                            # Insertion des données dans la base de données SQLite\n",
    "                            if self.start_date <= raw_date <= self.end_date:\n",
    "                                self.insert_into_db(article)\n",
    "                                #print(f\"Titre: {title}\\nLien: {link}\\nDate de publication: {formatted_date}\\nContenu: {text}\\n\")\n",
    "                            elif raw_date < self.start_date:\n",
    "                                print(f\"Article plus ancien que {self.start_date}. Arrêt du scraping.\")\n",
    "                                return\n",
    "                        except Exception as e:\n",
    "                            print(f\"Erreur lors de l'insertion dans la base de données : {e}\")\n",
    "                            continue  # Passer à l'article suivant en cas d'erreur\n",
    "\n",
    "                        time.sleep(0.5)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur lors du traitement de la page {i}: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur de traitement globale: {e}\")\n",
    "\n",
    "    def is_duplicate(self, link):\n",
    "        # Vérifier si l'article existe déjà dans la base de données\n",
    "        self.cursor.execute(\"SELECT link FROM cnil_articles WHERE link = ?\", (link,))\n",
    "        return self.cursor.fetchone() is not None\n",
    "\n",
    "class ECBScraper(BaseScraper):\n",
    "    def __init__(self, db_name='ecb.db'):\n",
    "        super().__init__(db_name, 'ecb_articles')\n",
    "        self.url_base = \"https://www.ecb.europa.eu/press/pr/date/html/index.en.html\"\n",
    "        self.options = webdriver.ChromeOptions()\n",
    "        self.options.add_argument('--headless')\n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "        self.options.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2})\n",
    "\n",
    "        self.driver = webdriver.Chrome(options=self.options)\n",
    "        self.consecutive_duplicates = 0  # Compteur de doublons consécutifs\n",
    "\n",
    "    def __del__(self):\n",
    "        super().__del__()\n",
    "        self.driver.quit()\n",
    "\n",
    "    def translate_text(self,text, dest_language='fr'):\n",
    "        if not text: \n",
    "            return text\n",
    "\n",
    "        translator = Translator()\n",
    "        try:\n",
    "            # Diviser le texte en segments de 5000 caractères\n",
    "            segments = [text[i:i+5000] for i in range(0, len(text), 5000)]\n",
    "            translated_segments = []\n",
    "            for segment in segments:\n",
    "                translation = translator.translate(segment, dest=dest_language)\n",
    "                translated_segments.append(translation.text)\n",
    "\n",
    "            # Concaténer les segments traduits\n",
    "            return '\\n'.join(translated_segments)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la traduction: {e}\")\n",
    "            print(text)\n",
    "            return text  # Retourner le texte original en cas d'erreur\n",
    "\n",
    "    def is_duplicate(self, article):\n",
    "        # Vérifier si l'article existe déjà dans la base de données\n",
    "        self.cursor.execute('SELECT COUNT(*) FROM ecb_articles WHERE link = ?', (article['URL'],))\n",
    "        return self.cursor.fetchone()[0] > 0\n",
    "    \n",
    "    def delete_last_articles(self, num_articles=1):\n",
    "        try:\n",
    "            for _ in range(num_articles):\n",
    "                self.cursor.execute(f'''\n",
    "                    DELETE FROM {self.table_name}\n",
    "                    WHERE id = (SELECT MIN(id) FROM {self.table_name})\n",
    "                ''')\n",
    "                self.conn.commit()\n",
    "                print(\"Dernier article supprimé de la table.\")\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"Erreur lors de la suppression de l'article : {e}\")\n",
    "\n",
    "    def scrape_site(self): \n",
    "        try:\n",
    "            self.driver.get(self.url_base)\n",
    "            for i in tqdm(range(2), desc=\"Scraping articles\"):\n",
    "                div_id = f\"snippet{i}\"\n",
    "                div_element = WebDriverWait(self.driver, 150).until(lambda d: d.find_element(By.ID, div_id))\n",
    "                self.driver.execute_script(\"arguments[0].scrollIntoView();\", div_element)\n",
    "\n",
    "                # Attendre que la classe de la div devienne \"loaded\"\n",
    "                WebDriverWait(self.driver, 300).until(lambda d: \"loaded\" in div_element.get_attribute(\"class\"))\n",
    "\n",
    "                # Extraire les informations une fois la div chargée\n",
    "                dts = div_element.find_elements(By.TAG_NAME, \"dt\")\n",
    "                dds = div_element.find_elements(By.TAG_NAME, \"dd\")\n",
    "\n",
    "                for dt, dd in zip(dts, dds):\n",
    "                    date_elements = dt.find_elements(By.CSS_SELECTOR, \"div.date\")\n",
    "                    link_elements = dd.find_elements(By.CSS_SELECTOR, \"div.title > a\")\n",
    "\n",
    "                    if date_elements and link_elements:\n",
    "                        try : \n",
    "                            raw_date = date_elements[0].text\n",
    "                        except : \n",
    "                            print(f\"Date non trouvé \")\n",
    "\n",
    "                        # Convertir la date en objet datetime et reformater\n",
    "                        try:\n",
    "                            article_date =parser.parse(raw_date)\n",
    "                            #article_date = datetime.strptime(raw_date, \"%d %B %Y\")\n",
    "                            # Vérifier si la date est antérieure à 2023\n",
    "                            if article_date.year <= 2022:\n",
    "                                print(article_date.year)\n",
    "                                continue  # Ignorer l'article \n",
    "                            date = article_date.strftime(\"%Y-%m-%d\")\n",
    "                        except :\n",
    "                            if raw_date != \"\":\n",
    "                                print(f\"Format de date non reconnu : {raw_date}\")\n",
    "                            continue\n",
    "\n",
    "                        first_link_element = link_elements[0]\n",
    "                        link = first_link_element.get_attribute('href')\n",
    "                        article = {'URL': link}\n",
    "                \n",
    "                        article['source']= \"ECB\"\n",
    "                        title = first_link_element.text\n",
    "                        article[\"Titre\"] = self.translate_text(title)\n",
    "                        article[\"Date\"] = date\n",
    "                        \n",
    "                        self.driver.execute_script(f\"window.open('{link}', 'newtab{i}');\")\n",
    "                        self.driver.switch_to.window(self.driver.window_handles[1])\n",
    "\n",
    "                        content = \"\"\n",
    "                        try:\n",
    "                            # Sélectionner tous les éléments <p> à l'intérieur de la div avec la classe 'section'\n",
    "                            p_elements = self.driver.find_elements(By.XPATH, \"//div[@class='section']//p[not(contains(@class, 'ecb-publicationDate')) and not(.//span[contains(@class, 'ecb-footnote-toggle')])]\")\n",
    "                            for index, p in enumerate(p_elements):\n",
    "                                # Supprimer la première phrase si elle commence par \"According to the\"\n",
    "                                if index == 0 and p.text.startswith(\"According to the\"):\n",
    "                                    continue\n",
    "                                content += p.text \n",
    "                        except NoSuchElementException:\n",
    "                            print(\"Élément non trouvé.\")\n",
    "                        content = self.translate_text(content)\n",
    "                        article[\"Contenu\"] = content\n",
    "                        \n",
    "                        #print(\"Article : \",article)\n",
    "                        if  content and date and title and link.startswith(\"https://www.ecb.europa.eu/press/\") :\n",
    "                            if not self.is_duplicate(article):\n",
    "                                try :\n",
    "                                    self.insert_into_db(article)\n",
    "                                    self.consecutive_duplicates = 0\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Erreur lors de l'insertion des données de l'article: {e}\")\n",
    "                            else : \n",
    "                                print(f\"Article déjà existant dans la base de données : {article['URL']}\")\n",
    "                                self.consecutive_duplicates += 1\n",
    "\n",
    "                        self.driver.close()\n",
    "                        self.driver.switch_to.window(self.driver.window_handles[0])\n",
    "                        \n",
    "                        # Condition pour arrêter le scraping si deux doublons consécutifs sont rencontrés\n",
    "                        if self.consecutive_duplicates >= 2:\n",
    "                            print(\"Deux doublons consécutifs rencontrés. Arrêt du scraping.\")\n",
    "                            return\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'accès à la page : {e}\")\n",
    "\n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "\n",
    "class TribuneScraper(BaseScraper):\n",
    "    def __init__(self, db_name='tribune.db'):\n",
    "        super().__init__(db_name, 'tribune_articles')\n",
    "        self.start_url = \"https://www.latribune.fr/Entreprises-secteurs.html\"\n",
    "        self.options = webdriver.ChromeOptions()\n",
    "        self.options.add_argument('--headless')\n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "        self.driver = webdriver.Chrome(options=self.options)\n",
    "        self.consecutive_duplicates = 0  # Compteur de doublons consécutifs\n",
    "\n",
    "    def __del__(self):\n",
    "        super().__del__()\n",
    "        self.driver.quit()\n",
    "\n",
    "    def is_duplicate(self, article):\n",
    "        # Vérifier si l'article existe déjà dans la base de données\n",
    "        self.cursor.execute('SELECT COUNT(*) FROM tribune_articles WHERE link = ?', (article['URL'],))\n",
    "        return self.cursor.fetchone()[0] > 0\n",
    "    \n",
    "    def delete_last_articles(self, num_articles=1):\n",
    "        try:\n",
    "            for _ in range(num_articles):\n",
    "                self.cursor.execute(f'''\n",
    "                    DELETE FROM {self.table_name}\n",
    "                    WHERE id = (SELECT MIN(id) FROM {self.table_name})\n",
    "                ''')\n",
    "                self.conn.commit()\n",
    "                print(\"Dernier article supprimé de la table.\")\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"Erreur lors de la suppression de l'article : {e}\")\n",
    "\n",
    "    def get_article_count(self, year):\n",
    "        # Récupérer le nombre d'articles basé sur l'année donnée\n",
    "        query = \"SELECT COUNT(*) FROM tribune_articles WHERE date LIKE ?\"\n",
    "        parameters = (f\"{year}%\",)\n",
    "        self.cursor.execute(query, parameters)\n",
    "        return self.cursor.fetchone()[0]\n",
    "    \n",
    "    def extract_urls_from_db(self,db_path):\n",
    "        \"\"\"Extrait les URL des articles d'une base de données SQLite et les retourne dans une liste.\"\"\"\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT url FROM articles WHERE url LIKE 'https://www.latribune.fr/%'\")  \n",
    "        links = [item[0] for item in cursor.fetchall()]\n",
    "        conn.close()\n",
    "        return links\n",
    "    \n",
    "    def scrap_tribune_hist(self,attempt=1, max_attempts=10):\n",
    "        try:\n",
    "            links = self.extract_urls_from_db(\"tribune_articles_list.sqlite3\")\n",
    "        \n",
    "            for link in tqdm(links,desc=\"Scraping des pages\"):\n",
    "                article = {'URL': link}\n",
    "                article['source']= \"La Tribune\"\n",
    "                if self.is_duplicate(article):\n",
    "                    print(f\"L'article '{link}' existe déjà dans la base de données.\")\n",
    "                    continue  # Passer à l'article suivant\n",
    "\n",
    "                # Naviguer vers chaque article\n",
    "                try : \n",
    "                    self.driver.get(link)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur lors du déplacement vers l'article: {e}\")\n",
    "\n",
    "                # Extraire le titre \n",
    "                try : \n",
    "                    title = self.driver.find_element(By.CSS_SELECTOR, 'h1[itemprop=\"Headline\"]').text\n",
    "                    article['Titre'] = title\n",
    "                except NoSuchElementException:\n",
    "                    print(\"Titre manquante dans cet article.\")\n",
    "\n",
    "                # Extraire la date \n",
    "                try : \n",
    "                    date_element = self.driver.find_element(By.XPATH, \"//div[@class='author-article-informations']/time\")\n",
    "                    date_time = datetime.strptime(date_element.get_attribute(\"datetime\").split('C')[0], '%Y-%m-%d')\n",
    "                    date = date_time.date()  # Obtient l'objet date\n",
    "                    article['Date']=date\n",
    "                except NoSuchElementException:\n",
    "                    print(\"Date manquante dans cet article.\")\n",
    "                    \n",
    "                # Extraire le contenu\n",
    "                try : \n",
    "                    # Extraire le contenu de la section \"chapo\"\n",
    "                    chapo_element = self.driver.find_element(By.XPATH, \"//section[@class='chapo']\")\n",
    "                    chapo_full_text = chapo_element.text\n",
    "\n",
    "                    # Identifier le texte à exclure\n",
    "                    if \"Écoutez cet article\" in chapo_full_text:\n",
    "                        chapo_text = chapo_full_text.split(\"Écoutez cet article\")[0].strip()\n",
    "                    elif \"Réservé aux abonnés\" in chapo_full_text:\n",
    "                        chapo_text = chapo_full_text.split(\"Réservé aux abonnés\")[0].strip()\n",
    "                    else:\n",
    "                        chapo_text = chapo_full_text\n",
    "                        \n",
    "                    # Extraire le contenu de l'article, en excluant les 'widgetlink'\n",
    "                    content_elements = self.driver.find_elements(By.XPATH, \"//div[@id='body-article']//*[self::p or self::blockquote or self::h2 or self::ul]\")\n",
    "                    content = \"\"\n",
    "                    for element in content_elements:\n",
    "                        if element.find_elements(By.CLASS_NAME, \"widgetlink\"):\n",
    "                            continue  # Ignore les éléments avec 'widgetlink'\n",
    "                        content += element.text + \" \"\n",
    "\n",
    "                    content = chapo_text + \" \" + content.strip()\n",
    "                    article[\"Contenu\"] = content\n",
    "                except NoSuchElementException:\n",
    "                    print(\"Contenu manquant dans cet article.\")\n",
    "\n",
    "                print(f\"Titre: {title}\\nLien: {link}\\nDate de publication: {date}\\nContenu: {content}\\n\")\n",
    "\n",
    "               \n",
    "                # Insérer les données dans la base de données\n",
    "                try :\n",
    "                    # Vérifier si l'article existe déjà dans la base de données\n",
    "                    if not self.is_duplicate(article):\n",
    "                        self.insert_into_db(article)\n",
    "                    else:\n",
    "                        print(f\"Article déjà existant dans la base de données : {article['URL']}\")                \n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur lors de l'insertion des données de l'article: {e}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Une erreur s'est produite lors du scraping initial: {e}\")\n",
    "            if attempt < max_attempts:\n",
    "                self.scrap_tribune_hist(attempt + 1, max_attempts)\n",
    "            else:\n",
    "                print(\"Nombre maximum de tentatives atteint. Arrêt du script.\")\n",
    "        finally:\n",
    "            # Fermer le navigateur après l'extraction\n",
    "            self.driver.quit()\n",
    "            print(attempt)\n",
    "\n",
    "    def scrap_tribune_month(self,month,attempt=1, max_attempts=10):\n",
    "        base_url = \"https://www.latribune.fr\"        \n",
    "        start_url = f\"{base_url}/entreprises-finance-11/{month}-2023/page-1\"\n",
    "\n",
    "        try:\n",
    "            # Ouvrir la page web\n",
    "            self.driver.get(start_url)\n",
    "            print(start_url)\n",
    "            wait = WebDriverWait(self.driver, 10)\n",
    "\n",
    "            while True : \n",
    "                # Trouver tous les articles\n",
    "                wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article[class*='article-wrapper']\")))\n",
    "\n",
    "                articles = self.driver.find_elements(By.CSS_SELECTOR, \"article[class*='article-wrapper']\")\n",
    "                print(len(articles))\n",
    "\n",
    "                # Extraire les titres et les liens\n",
    "                links = []\n",
    "                \n",
    "                for article in articles:\n",
    "                    try:\n",
    "                        title_element = article.find_element(By.XPATH, \".//h2/a\")\n",
    "                        title = title_element.text\n",
    "                        title = re.sub(r'^Exclusif\\s*', '', title)\n",
    "\n",
    "                        link = title_element.get_attribute(\"href\")\n",
    "                        links.append((title, link))\n",
    "                    except NoSuchElementException:\n",
    "                        print(\"Titre ou lien manquant dans cet article.\")\n",
    "                \n",
    "                for title, link in links:\n",
    "                    article = {'URL': link}\n",
    "                    article['source']= \"La Tribune\"\n",
    "                    article['Titre'] = title\n",
    "                    # Vérifier si l'article existe déjà dans la base de données\n",
    "                    if self.is_duplicate(article):\n",
    "                            print(f\"L'article '{title}' existe déjà dans la base de données.\")\n",
    "                            continue  # Passer à l'article suivant\n",
    "\n",
    "                    # Naviguer vers chaque article\n",
    "                    try : \n",
    "                        self.driver.get(link)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Erreur lors du déplacement vers l'article: {e}\")\n",
    "\n",
    "                    # Extraire la date \n",
    "                    try : \n",
    "                        # Extraire la date de publication\n",
    "                        date_element = self.driver.find_element(By.XPATH, \"//div[@class='author-article-informations']/time\")\n",
    "                        date_time = datetime.strptime(date_element.get_attribute(\"datetime\").split('C')[0], '%Y-%m-%d')\n",
    "                        date = date_time.date()  # Obtient l'objet date\n",
    "                        article['Date']=date\n",
    "                    except NoSuchElementException:\n",
    "                        print(\"Date manquante dans cet article.\")\n",
    "                    \n",
    "                    # Extraire le contenu\n",
    "                    try : \n",
    "                        # Extraire le contenu de la section \"chapo\"\n",
    "                        chapo_element = self.driver.find_element(By.XPATH, \"//section[@class='chapo']\")\n",
    "                        chapo_full_text = chapo_element.text\n",
    "\n",
    "                        # Identifier le texte à exclure\n",
    "                        if \"Écoutez cet article\" in chapo_full_text:\n",
    "                            chapo_text = chapo_full_text.split(\"Écoutez cet article\")[0].strip()\n",
    "                        elif \"Réservé aux abonnés\" in chapo_full_text:\n",
    "                            chapo_text = chapo_full_text.split(\"Réservé aux abonnés\")[0].strip()\n",
    "                        else:\n",
    "                            chapo_text = chapo_full_text\n",
    "                        # Extraire le reste du contenu de l'article\n",
    "                        # Extraire le contenu de l'article, en excluant les 'widgetlink'\n",
    "                        content_elements = self.driver.find_elements(By.XPATH, \"//div[@id='body-article']//*[self::p or self::blockquote or self::h2 or self::ul]\")\n",
    "                        content = \"\"\n",
    "                        for element in content_elements:\n",
    "                            if element.find_elements(By.CLASS_NAME, \"widgetlink\"):\n",
    "                                continue  # Ignore les éléments avec 'widgetlink'\n",
    "                            content += element.text + \" \"\n",
    "\n",
    "                        content = chapo_text + \" \" + content.strip()\n",
    "                        article[\"Contenu\"] = content\n",
    "                    except NoSuchElementException:\n",
    "                        print(\"Contenu manquant dans cet article.\")\n",
    "\n",
    "                    #print(f\"Titre: {title}\\nLien: {link}\\nDate de publication: {date}\\nContenu: {content}\\n\")\n",
    "                    source = \"La tribune\"\n",
    "                    # Revenir à la page principale\n",
    "                    self.driver.get(start_url)\n",
    "                    \n",
    "                    # Insérer les données dans la base de données\n",
    "                    try :\n",
    "                        # Vérifier si l'article existe déjà dans la base de données\n",
    "                        if not self.is_duplicate(article):\n",
    "                            self.insert_into_db(article)\n",
    "                            self.consecutive_duplicates = 0 \n",
    "                        else:\n",
    "                            print(f\"Article déjà existant dans la base de données : {article['URL']}\")\n",
    "                            self.consecutive_duplicates += 1 \n",
    "                        # Condition pour arrêter le scraping si deux doublons consécutifs sont rencontrés\n",
    "                        if self.consecutive_duplicates >= 2:\n",
    "                            print(\"Deux doublons consécutifs rencontrés. Arrêt du scraping.\")\n",
    "                            return\n",
    "                    except Exception as e:\n",
    "                        print(f\"Erreur lors de l'extraction ou de l'insertion des données de l'article: {e}\")\n",
    "                \n",
    "                # Trouver le lien de la page suivante\n",
    "                try:\n",
    "                    wait = WebDriverWait(self.driver, 10)\n",
    "                    next_page_element = wait.until(EC.presence_of_element_located((By.XPATH, \"//ul[@class='pagination-archive pages']/li[@class='current']/following-sibling::li[1]/a\")))\n",
    "                    next_page_url = next_page_element.get_attribute('href')\n",
    "                    self.driver.get(next_page_url)\n",
    "                except Exception:\n",
    "                    # Si aucun lien de page suivante n'est trouvé, sortir de la boucle\n",
    "                    print(\"next page impossible\")\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"Une erreur s'est produite lors du scraping initial: {e}\")\n",
    "            if attempt < max_attempts:\n",
    "                self.scrap_tribune_month(attempt + 1, max_attempts)\n",
    "            else:\n",
    "                print(\"Nombre maximum de tentatives atteint. Arrêt du script.\")\n",
    "        finally:\n",
    "            # Fermer le navigateur après l'extraction\n",
    "            self.driver.quit()\n",
    "            \n",
    "            print(attempt)\n",
    "\n",
    "    def scrape_site(self, attempt=1, max_attempts=10):\n",
    "            count = self.get_article_count(2023)\n",
    "            print(count)\n",
    "            if count <2500 : \n",
    "                print(\"Get Historical Data\")\n",
    "                try :\n",
    "                    self.scrap_tribune_hist(1,10)\n",
    "                except Exception as e :\n",
    "                    print(f\"Impossible de scraper les données historiques: {e}\")\n",
    "\n",
    "                print(\"Get Historical Data By Month\")\n",
    "                try : \n",
    "                    months = [\"janvier\", \"fevrier\", \"mars\", \"avril\", \"mai\", \"juin\", \"juillet\", \"aout\", \"septembre\", \"octobre\", \"novembre\", \"decembre\"]\n",
    "                    for month in months : \n",
    "                        print(month)\n",
    "                        self.scrap_tribune_month(month,1, 30)   \n",
    "                except Exception as e :\n",
    "                    print(f\"Impossible de scraper les données historiques mensuelles: {e}\")\n",
    "\n",
    "            try:\n",
    "                self.driver.get(self.start_url)\n",
    "                wait = WebDriverWait(self.driver, 10)\n",
    "\n",
    "                while True : \n",
    "                    # Trouver tous les articles\n",
    "                    wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article[class*='article-wrapper']\")))\n",
    "\n",
    "                    articles = self.driver.find_elements(By.CSS_SELECTOR, \"article[class*='article-wrapper']\")\n",
    "\n",
    "                    # Extraire les titres et les liens\n",
    "                    links = []\n",
    "                    \n",
    "                    for article in articles:\n",
    "                        try:\n",
    "                            title_element = article.find_element(By.XPATH, \".//h2/a\")\n",
    "                            title = title_element.text\n",
    "                            title = re.sub(r'^Exclusif\\s*', '', title)\n",
    "\n",
    "                            link = title_element.get_attribute(\"href\")\n",
    "                            links.append((title, link))\n",
    "                        except NoSuchElementException:\n",
    "                            print(\"Titre ou lien manquant dans cet article.\")\n",
    "                    \n",
    "                    for title, link in links:\n",
    "                        article = {'URL': link}\n",
    "                        article['source']= \"La Tribune\"\n",
    "                        article['Titre'] = title\n",
    "                        # Vérifier si l'article existe déjà dans la base de données\n",
    "                        #cursor.execute(\"SELECT id FROM tribune_articles WHERE link = ?\", (link,))\n",
    "                        #if cursor.fetchone():\n",
    "                        if self.is_duplicate(article):\n",
    "                            print(f\"L'article '{title}' existe déjà dans la base de données.\")\n",
    "                            continue  # Passer à l'article suivant\n",
    "\n",
    "                        # Naviguer vers chaque article\n",
    "                        try : \n",
    "                            self.driver.get(link)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Erreur lors du déplacement vers l'article: {e}\")\n",
    "\n",
    "                        # Extraire la date \n",
    "                        try : \n",
    "                            # Extraire la date de publication\n",
    "                            date_element = self.driver.find_element(By.XPATH, \"//div[@class='author-article-informations']/time\")\n",
    "                            date_time = datetime.strptime(date_element.get_attribute(\"datetime\").split('C')[0], '%Y-%m-%d')\n",
    "                            date = date_time.date()  # Obtient l'objet date\n",
    "                            article['Date']=date\n",
    "                        except NoSuchElementException:\n",
    "                            print(\"Date manquante dans cet article.\")\n",
    "                        \n",
    "                        # Extraire le contenu\n",
    "                        try : \n",
    "                            # Extraire le contenu de la section \"chapo\"\n",
    "                            chapo_element = self.driver.find_element(By.XPATH, \"//section[@class='chapo']\")\n",
    "                            chapo_full_text = chapo_element.text\n",
    "\n",
    "                            # Identifier le texte à exclure\n",
    "                            if \"Écoutez cet article\" in chapo_full_text:\n",
    "                                chapo_text = chapo_full_text.split(\"Écoutez cet article\")[0].strip()\n",
    "                            elif \"Réservé aux abonnés\" in chapo_full_text:\n",
    "                                chapo_text = chapo_full_text.split(\"Réservé aux abonnés\")[0].strip()\n",
    "                            else:\n",
    "                                chapo_text = chapo_full_text\n",
    "                            # Extraire le reste du contenu de l'article\n",
    "                            # Extraire le contenu de l'article, en excluant les 'widgetlink'\n",
    "                            content_elements = self.driver.find_elements(By.XPATH, \"//div[@id='body-article']//*[self::p or self::blockquote or self::h2 or self::ul]\")\n",
    "                            content = \"\"\n",
    "                            for element in content_elements:\n",
    "                                if element.find_elements(By.CLASS_NAME, \"widgetlink\"):\n",
    "                                    continue  # Ignore les éléments avec 'widgetlink'\n",
    "                                content += element.text + \" \"\n",
    "\n",
    "                            content = chapo_text + \" \" + content.strip()\n",
    "                            article[\"Contenu\"] = content\n",
    "                        except NoSuchElementException:\n",
    "                            print(\"Contenu manquant dans cet article.\")\n",
    "\n",
    "                        #print(f\"Titre: {title}\\nLien: {link}\\nDate de publication: {date}\\nContenu: {content}\\n\")\n",
    "                        \n",
    "                        # Revenir à la page principale\n",
    "                        self.driver.get(self.start_url)\n",
    "                        \n",
    "                        # Insérer les données dans la base de données\n",
    "                        try :\n",
    "                            # Vérifier si l'article existe déjà dans la base de données\n",
    "                            if not self.is_duplicate(article):\n",
    "                                self.insert_into_db(article)\n",
    "                                self.consecutive_duplicates = 0 \n",
    "                            else:\n",
    "                                print(f\"Article déjà existant dans la base de données : {article['URL']}\")\n",
    "                                self.consecutive_duplicates += 1 \n",
    "                            # Condition pour arrêter le scraping si deux doublons consécutifs sont rencontrés\n",
    "                            if self.consecutive_duplicates >= 2:\n",
    "                                print(\"Deux doublons consécutifs rencontrés. Arrêt du scraping.\")\n",
    "                                return\n",
    "                        except Exception as e:\n",
    "                            print(f\"Erreur lors de l'extraction ou de l'insertion des données de l'article: {e}\")\n",
    "                    \n",
    "                    # Trouver le lien de la page suivante\n",
    "                    try:\n",
    "                        wait = WebDriverWait(self.driver, 10)\n",
    "                        next_page_element = wait.until(EC.presence_of_element_located((By.XPATH, \"//ul[@class='pagination-archive pages']/li[@class='current']/following-sibling::li[1]/a\")))\n",
    "                        next_page_url = next_page_element.get_attribute('href')\n",
    "                        self.driver.get(next_page_url)\n",
    "                    except Exception:\n",
    "                        # Si aucun lien de page suivante n'est trouvé, sortir de la boucle\n",
    "                        print(\"next page impossible\")\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(f\"Une erreur s'est produite lors du scraping initial: {e}\")\n",
    "                if attempt < max_attempts:\n",
    "                    print(\"OUI\")\n",
    "                    self.scrape_site(attempt + 1, max_attempts)\n",
    "                else:\n",
    "                    print(\"Nombre maximum de tentatives atteint. Arrêt du script.\")\n",
    "            finally:\n",
    "                self.driver.quit()\n",
    "\n",
    "class BDFScraper(BaseScraper):\n",
    "    def __init__(self, db_name='bdf.db'):\n",
    "        super().__init__(db_name, 'bdf_articles')\n",
    "        self.url_base = \"https://www.banque-france.fr/fr/actualites?page=0\"\n",
    "        self.options = webdriver.ChromeOptions()\n",
    "        self.options.add_argument('--headless')\n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "        #self.options.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2})\n",
    "\n",
    "        self.driver = webdriver.Chrome(options=self.options)\n",
    "        self.consecutive_duplicates = 0  # Compteur de doublons consécutifs\n",
    "\n",
    "    def __del__(self):\n",
    "        super().__del__()\n",
    "        self.driver.quit()\n",
    "\n",
    "    def is_duplicate(self, article):\n",
    "        # Vérifier si l'article existe déjà dans la base de données\n",
    "        self.cursor.execute('SELECT COUNT(*) FROM bdf_articles WHERE link = ?', (article['URL'],))\n",
    "        return self.cursor.fetchone()[0] > 0\n",
    "    \n",
    "    def delete_last_articles(self, num_articles=1):\n",
    "        try:\n",
    "            for _ in range(num_articles):\n",
    "                self.cursor.execute(f'''\n",
    "                    DELETE FROM {self.table_name}\n",
    "                    WHERE id = (SELECT MIN(id) FROM {self.table_name})\n",
    "                ''')\n",
    "                self.conn.commit()\n",
    "                print(\"Dernier article supprimé de la table.\")\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"Erreur lors de la suppression de l'article : {e}\")\n",
    "\n",
    "    def scrape_site(self):\n",
    "         # Réglage de la locale pour interpréter correctement les noms de mois en français\n",
    "        locale.setlocale(locale.LC_TIME, 'fr_FR')\n",
    "        self.driver.get(self.url_base)\n",
    "        last_page = int(self.driver.find_element(By.CSS_SELECTOR, \"ul.pb-0.pt-lg-8.pt-6.d-flex.align-items-center.pagination.pager__items.js-pager__items li.page-item.d-md-flex.d-none.pager__item.pager__item--last\").text)\n",
    "        #print(\"last page : \",last_page)\n",
    "\n",
    "        # Création d'un onglet supplémentaire pour les détails des articles\n",
    "        self.driver.execute_script(\"window.open();\")\n",
    "        detail_tab = self.driver.window_handles[1]\n",
    "\n",
    "        for i in tqdm(range(last_page), desc=\"Scraping des pages\"):\n",
    "            try:\n",
    "                self.driver.switch_to.window(self.driver.window_handles[0])\n",
    "                self.driver.get(f\"https://www.banque-france.fr/fr/actualites?page={i}\")\n",
    "                cards = self.driver.find_elements(By.CLASS_NAME, \"col.d-flex.d-flex\")\n",
    "\n",
    "                for card_index in range(len(cards)):\n",
    "                    try:\n",
    "                        self.driver.switch_to.window(self.driver.window_handles[0])\n",
    "                        cards = self.driver.find_elements(By.CLASS_NAME, \"col.d-flex.d-flex\")\n",
    "                        card = cards[card_index]\n",
    "\n",
    "                        if card.text.strip():\n",
    "                            link = card.find_element(By.CLASS_NAME, \"d-flex.align-items-center.text-underline-hover.h6.text-primary-black.mb-lg-0.my-1\").get_attribute('href')\n",
    "                            article = {'URL': link}\n",
    "                            article['source']= \"BDF\"\n",
    "                            if self.is_duplicate(article):\n",
    "                                break  # Passer à l'article suivant\n",
    "\n",
    "                            # Conversion de la date de l'article\n",
    "                            try:\n",
    "                                raw_date = card.find_element(By.CSS_SELECTOR, \"small.d-flex.mt-auto.text-grey-l6.fw-semibold.pt-lg-6.mt-2\").text\n",
    "\n",
    "                                raw_date = datetime.strptime(raw_date, \"%d %B %Y\")\n",
    "                                date = raw_date.strftime(\"%Y-%m-%d\")  # Format ISO pour stockage \n",
    "                                article[\"Date\"] = date\n",
    "                                # Vérification de la date <2 023\n",
    "                                if raw_date.year < 2023:\n",
    "                                    print(f\"Article antérieur à 2023 détecté : {date}, arrêt du scraping. Lien de l'article : {link}\")\n",
    "                                    break  # Sortie anticipée si un article antérieur à 2023 est trouvé\n",
    "\n",
    "                            except ValueError:\n",
    "                                print(f\"Format de date non reconnu : {raw_date}\")\n",
    "                                continue                        \n",
    "                            \n",
    "                            self.driver.switch_to.window(detail_tab)\n",
    "                            self.driver.get(link)\n",
    "                            content = \"\"\n",
    "                            try:\n",
    "                                more_info_element = self.driver.find_element(By.CLASS_NAME, \"py-lg-5.py-5.border-0.border-grey-l3.border-top.border-bottom\")\n",
    "                                if more_info_element:\n",
    "                                    \n",
    "                                    more_info_link = more_info_element.find_element(By.TAG_NAME, \"a\").get_attribute('href')\n",
    "                                    \n",
    "                                    self.driver.get(more_info_link)  # Aller à la page de l'article complet\n",
    "                                    \n",
    "                                    # Accept cookies\n",
    "                                    try:\n",
    "                                        # Localiser le bouton d'acceptation des cookies par son id\n",
    "                                        cookie_button = WebDriverWait(self.driver, 10).until(\n",
    "                                            EC.element_to_be_clickable((By.ID, \"footer_tc_privacy_button\"))\n",
    "                                        )\n",
    "                                        cookie_button.click()\n",
    "                                    except TimeoutException:\n",
    "                                        # Le bouton n'apparaît pas dans le délai imparti\n",
    "                                        print(\"Le bouton d'acceptation des cookies n'a pas été trouvé dans le temps imparti.\")\n",
    "                                    except NoSuchElementException:\n",
    "                                        # Le bouton n'existe pas sur la page\n",
    "                                        print(\"Le bouton d'acceptation des cookies n'est pas présent sur la page.\")\n",
    "                                    except Exception as e:\n",
    "                                        print(f\"Erreur lors de la gestion des cookies: {e}\")\n",
    "\n",
    "                                    content_elements = self.driver.find_elements(By.CSS_SELECTOR, 'p, h2')\n",
    "\n",
    "                                    # Ensuite, filtrez les éléments <p> avec la classe indésirable\n",
    "                                    filtered_content = []\n",
    "                                    for element in content_elements:\n",
    "                                        # Vérifier si c'est un élément <p> ou <h2> indésirable\n",
    "                                        if (element.tag_name == 'p' and \"fs-4 mt-0 fw-semibold mb-4 text-center mb-lg-5 text-white d-flex justify-content-center\" in element.get_attribute('class')) or (element.tag_name == 'h2' and \"chapter-container h5\" in element.get_attribute('class')):\n",
    "                                            continue  # Exclure cet élément\n",
    "                                        filtered_content.append(element.text)  # Ajouter le texte de l'élément <p> ou <h2> au contenu\n",
    "\n",
    "                                        content = \"\\n\".join([text for text in filtered_content if text.strip()])\n",
    "                                        # Supprimer la première phrase si elle commence par \"Billet\" ou \"Bulletin\"\n",
    "                                        if re.match(r'Bulletin No\\. \\d+, article \\d+\\.', content):\n",
    "                                            content = re.sub(r'Bulletin No\\. \\d+, article \\d+\\.', '', content, count=1).strip()\n",
    "                                        elif content.startswith((\"Billet\", \"Bulletin\")):\n",
    "                                            content = re.sub(r'^.*?\\.', '', content, count=1).strip()\n",
    "\n",
    "                                        if content.startswith((\"Billet\", \"Bulletin\")):\n",
    "                                            content = re.sub(r'^.*?\\.', '', content, count=1).strip()\n",
    "\n",
    "                                    \n",
    "                            except NoSuchElementException:\n",
    "                                pass  # Si l'élément n'est pas trouvé, restez sur la page actuelle\n",
    "\n",
    "                            if content ==\"\":\n",
    "                                content = \"\\n\".join([p.text for p in self.driver.find_elements(By.CLASS_NAME, 'field__item') if p.find_elements(By.TAG_NAME, 'picture') == [] and 'ratio d-flex  position-relative' not in p.get_attribute('class') and len(p.text) >= 50 and not p.text.lower().startswith('source :') and not p.text.lower().startswith('sources :')])  \n",
    "                            article[\"Contenu\"]= content\n",
    "                            title = self.driver.find_element(By.TAG_NAME, \"h1\").text\n",
    "                            article[\"Titre\"] = title\n",
    "                            print(f\"Titre: {title}\\nLien: {more_info_link}\\n{link}\\nDate de publication: {date}\\nContenu: {content}\\n\")\n",
    "\n",
    "                            try :\n",
    "                                self.insert_into_db(article)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Erreur lors de l'extraction ou de l'insertion des données de l'article: {e}\")\n",
    "                                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Erreur lors du traitement de la carte {card_index} sur la page {i}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors du traitement de la page {i}: {e}\")\n",
    "        \n",
    "        self.driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #amf_scraper = AmfScraper()\n",
    "    #amf_scraper.scrape_site()\n",
    "    #amf_scraper.delete_last_articles(2)\n",
    "\n",
    "    #cnil_scraper = CnilScraper()\n",
    "    #cnil_scraper.scrape_site() \n",
    "\n",
    "    #ecb_scraper = ECBScraper()\n",
    "    #ecb_scraper.scrape_site()\n",
    "    #ecb_scraper.delete_last_articles(1)\n",
    "   \n",
    "    #tribune_scraper = TribuneScraper()\n",
    "    #tribune_scraper.scrape_site()\n",
    "    #tribune_scraper.delete_last_articles(1)\n",
    "    #tribune_scraper.scrape_site()\n",
    "\n",
    "    #bdf_scraper = BDFScraper()\n",
    "    #bdf_scraper.scrape_site()\n",
    "\n",
    "    # Exemple d'utilisation de la fonction fusion\n",
    "    #chemins_bdd_existants = ['amf.db', 'tribune.db', 'ecb.db', 'bdf.db', 'cnil.db']\n",
    "    #chemin_nouvelle_bdd = './all.db'\n",
    "    #BaseScraper.fusionner_bases_de_donnees(chemins_bdd_existants, chemin_nouvelle_bdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EYenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
